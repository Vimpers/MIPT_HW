{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DEBUG: Checking if Pipeline(dataset_name=GSE68849) is complete\n",
      "DEBUG: Checking if ProcessFiles(dataset_name=GSE68849) is complete\n",
      "INFO: Informed scheduler that task   Pipeline_GSE68849_e34547c091   has status   PENDING\n",
      "DEBUG: Checking if ExtractTarFile(dataset_name=GSE68849) is complete\n",
      "INFO: Informed scheduler that task   ProcessFiles_GSE68849_e34547c091   has status   PENDING\n",
      "DEBUG: Checking if DownloadDataset(dataset_name=GSE68849) is complete\n",
      "INFO: Informed scheduler that task   ExtractTarFile_GSE68849_e34547c091   has status   PENDING\n",
      "INFO: Informed scheduler that task   DownloadDataset_GSE68849_e34547c091   has status   DONE\n",
      "INFO: Done scheduling tasks\n",
      "INFO: Running Worker with 1 processes\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 3\n",
      "INFO: [pid 11936] Worker Worker(salt=9499785598, workers=1, host=DESKTOP-EOU1OB8, username=Petroo, pid=11936) running   ExtractTarFile(dataset_name=GSE68849)\n",
      "INFO: [pid 11936] Worker Worker(salt=9499785598, workers=1, host=DESKTOP-EOU1OB8, username=Petroo, pid=11936) done      ExtractTarFile(dataset_name=GSE68849)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   ExtractTarFile_GSE68849_e34547c091   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 2\n",
      "INFO: [pid 11936] Worker Worker(salt=9499785598, workers=1, host=DESKTOP-EOU1OB8, username=Petroo, pid=11936) running   ProcessFiles(dataset_name=GSE68849)\n",
      "INFO: [pid 11936] Worker Worker(salt=9499785598, workers=1, host=DESKTOP-EOU1OB8, username=Petroo, pid=11936) done      ProcessFiles(dataset_name=GSE68849)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   ProcessFiles_GSE68849_e34547c091   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Pending tasks: 1\n",
      "INFO: [pid 11936] Worker Worker(salt=9499785598, workers=1, host=DESKTOP-EOU1OB8, username=Petroo, pid=11936) running   Pipeline(dataset_name=GSE68849)\n",
      "INFO: [pid 11936] Worker Worker(salt=9499785598, workers=1, host=DESKTOP-EOU1OB8, username=Petroo, pid=11936) done      Pipeline(dataset_name=GSE68849)\n",
      "DEBUG: 1 running tasks, waiting for next task to finish\n",
      "INFO: Informed scheduler that task   Pipeline_GSE68849_e34547c091   has status   DONE\n",
      "DEBUG: Asking scheduler for work...\n",
      "DEBUG: Done\n",
      "DEBUG: There are no more tasks to run at this time\n",
      "INFO: Worker Worker(salt=9499785598, workers=1, host=DESKTOP-EOU1OB8, username=Petroo, pid=11936) was stopped. Shutting down Keep-Alive thread\n",
      "INFO: \n",
      "===== Luigi Execution Summary =====\n",
      "\n",
      "Scheduled 4 tasks of which:\n",
      "* 1 complete ones were encountered:\n",
      "    - 1 DownloadDataset(dataset_name=GSE68849)\n",
      "* 3 ran successfully:\n",
      "    - 1 ExtractTarFile(dataset_name=GSE68849)\n",
      "    - 1 Pipeline(dataset_name=GSE68849)\n",
      "    - 1 ProcessFiles(dataset_name=GSE68849)\n",
      "\n",
      "This progress looks :) because there were no failed tasks or missing dependencies\n",
      "\n",
      "===== Luigi Execution Summary =====\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import tarfile\n",
    "import gzip\n",
    "import shutil\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "import luigi\n",
    "\n",
    "from luigi.util import requires\n",
    "from datetime import datetime\n",
    "\n",
    "# загружаем датасет\n",
    "class DownloadDataset(luigi.Task):\n",
    "    dataset_name = luigi.Parameter()\n",
    "\n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(f\"C:/Users/Petroo/Desktop/dz/{self.dataset_name}/GSE68849_RAW.tar\")\n",
    "    \n",
    "    def run(self):\n",
    "        os.makedirs(os.path.dirname(self.output().path), exist_ok=True)\n",
    "        url = f\"https://www.ncbi.nlm.nih.gov/geo/download/?acc={self.dataset_name}&format=file\"\n",
    "        subprocess.run([\"curl\", \"-o\", self.output().path, url], check=True)\n",
    "\n",
    "# извлекаем файлы из архива\n",
    "class ExtractTarFile(luigi.Task):\n",
    "    dataset_name = luigi.Parameter()\n",
    "\n",
    "    def requires(self):\n",
    "        return DownloadDataset(self.dataset_name)\n",
    "    \n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(f\"C:/Users/Petroo/Desktop/dz/{self.dataset_name}/extracted\")\n",
    "    \n",
    "    def run(self):\n",
    "        os.makedirs(self.output().path, exist_ok=True)\n",
    "        with tarfile.open(self.input().path, \"r\") as tar:\n",
    "            tar.extractall(path=self.output().path)\n",
    "\n",
    "        folder_counter = 1\n",
    "        for root, _, files in os.walk(self.output().path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".gz\"):\n",
    "                    gz_path = os.path.join(root, file)\n",
    "                    folder_name = f\"file_{folder_counter}\"\n",
    "                    folder_counter += 1\n",
    "                    extract_folder = os.path.join(root, folder_name)\n",
    "                    os.makedirs(extract_folder, exist_ok=True)\n",
    "    \n",
    "                    extracted_file_path = os.path.join(extract_folder, os.path.basename(gz_path[:-3]))\n",
    "                    with gzip.open(gz_path, 'rb') as f_in:\n",
    "                        with open(extracted_file_path, 'wb') as f_out:\n",
    "                            shutil.copyfileobj(f_in, f_out)\n",
    "\n",
    "                    os.remove(gz_path)\n",
    "\n",
    "#  обработка файлов\n",
    "@requires(ExtractTarFile)\n",
    "class ProcessFiles(luigi.Task):\n",
    "    dataset_name = luigi.Parameter()\n",
    "\n",
    "    def requires(self):\n",
    "        return ExtractTarFile(self.dataset_name)\n",
    "    \n",
    "    def output(self):\n",
    "        return luigi.LocalTarget(f\"C:/Users/Petroo/Desktop/dz/{self.dataset_name}/processed/timestamp.txt\")\n",
    "    \n",
    "    def run(self):\n",
    "        base_processed_path = f\"data/{self.dataset_name}/processed\"\n",
    "\n",
    "        for root, dirs, files in os.walk(self.input().path):\n",
    "            for file in files:\n",
    "                if file.endswith(\".txt\"):\n",
    "                    file_path = os.path.join(root, file)\n",
    "                    relative_path = os.path.relpath(root, self.input().path)\n",
    "                    processed_file_directory = os.path.join(base_processed_path, relative_path)\n",
    "                    os.makedirs(processed_file_directory, exist_ok=True)\n",
    "                    self.process_txt_file(file_path, processed_file_directory)\n",
    "\n",
    "        with self.output().open('w') as f:\n",
    "            f.write(datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "\n",
    "    def process_txt_file(self, filepath, output_directory):\n",
    "        section_data = {}\n",
    "        current_section = None\n",
    "        buffer = io.StringIO()\n",
    "\n",
    "        with open(filepath, 'r') as file:\n",
    "            for line in file:\n",
    "                if line.startswith('['):\n",
    "                    if current_section:\n",
    "                        buffer = self.process_section(buffer, \n",
    "                                                      filepath, \n",
    "                                                      current_section, \n",
    "                                                      section_data, \n",
    "                                                      output_directory)\n",
    "                        \n",
    "                    current_section = line.strip('[]\\n')\n",
    "                else:\n",
    "                    print(line, file=buffer)\n",
    "\n",
    "            if current_section:\n",
    "                buffer = self.process_section(buffer, \n",
    "                                              filepath, \n",
    "                                              current_section, \n",
    "                                              section_data, \n",
    "                                              output_directory)\n",
    "\n",
    "        for section_name, df in section_data.items():\n",
    "            full_output_file_path = os.path.join(output_directory, section_name)\n",
    "            df.to_csv(full_output_file_path, index=False)\n",
    "            \n",
    "            if \"Probes\" in section_name:\n",
    "                reduced_df = df.drop(columns=['Definition', \n",
    "                                              'Ontology_Component', \n",
    "                                              'Ontology_Process', \n",
    "                                              'Ontology_Function', \n",
    "                                              'Synonyms', \n",
    "                                              'Obsolete_Probe_Id', \n",
    "                                              'Probe_Sequence'])\n",
    "                \n",
    "                reduced_file_name = f\"reduced_{section_name}\"\n",
    "                reduced_output_file_path = os.path.join(output_directory, reduced_file_name)\n",
    "                reduced_df.to_csv(reduced_output_file_path, index=False)\n",
    "\n",
    "    def process_section(self, buffer, filepath, current_section, section_data, output_directory):\n",
    "        buffer.seek(0)\n",
    "        df = pd.read_csv(buffer, sep='\\t', header=0)\n",
    "        section_name = f\"{os.path.splitext(os.path.basename(filepath))[0]}_{current_section}.csv\"\n",
    "        section_data[section_name] = df\n",
    "        return io.StringIO()\n",
    "\n",
    "class Pipeline(luigi.WrapperTask):\n",
    "    \n",
    "    dataset_name = luigi.Parameter(default=\"GSE68849\")\n",
    "\n",
    "    def requires(self):\n",
    "        return ProcessFiles(dataset_name=self.dataset_name)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    luigi.build([Pipeline()], local_scheduler=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dz/\n",
      "    GSE68849_RAW.tar\n",
      "    GSE68849/\n",
      "        GSE68849_RAW.tar\n",
      "        extracted/\n",
      "            file_1/\n",
      "                GPL10558_HumanHT-12_V4_0_R1_15002873_B.txt\n",
      "            file_2/\n",
      "                GPL10558_HumanHT-12_V4_0_R2_15002873_B.txt\n",
      "        processed/\n",
      "            timestamp.txt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def list_files(startpath):\n",
    "    for root, dirs, files in os.walk(startpath):\n",
    "        level = root.replace(startpath, '').count(os.sep)\n",
    "        indent = ' ' * 4 * (level)\n",
    "        print('{}{}/'.format(indent, os.path.basename(root)))\n",
    "        subindent = ' ' * 4 * (level + 1)\n",
    "        for f in files:\n",
    "            print('{}{}'.format(subindent, f))\n",
    "\n",
    "# Путь к корневой папке, где находятся  данные\n",
    "start_path = \"C:/Users/Petroo/Desktop/dz\"\n",
    "list_files(start_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
